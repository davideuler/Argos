{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hae8NChJE8B_"
      },
      "source": [
        "# Using Claude 3 to transform a video tutorial in a blog post\n",
        "\n",
        "This notebook provides a baseline to reproduce Anthropic's solution to Karpathy's challenge of converting a video tutorial in a blog post. See associated Medium article [here](https://medium.com/@ya-lb/using-claude-3-to-transform-a-video-tutorial-in-a-blog-post-d2c1e04e7a7b).\n",
        "\n",
        "### Original data\n",
        "\n",
        "- Karpathy's video tutorial on tokenization : https://www.youtube.com/watch?v=zduSFxRajkE\n",
        "- Hand-written tutorial summary : https://github.com/karpathy/minbpe/blob/master/lecture.md\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezEbNf6YF2ri"
      },
      "source": [
        "# Install and import libraries\n",
        "\n",
        "- `pytube`: used to download a Youtube video\n",
        "- `youtube-transcript-api`: used to directly download the video transcript from Youtube, if available\n",
        "- `faster_whisper`: used to get transcript from audio\n",
        "- `anthropic`: used to access Claude 3.0 large multimodal model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZDNNuLFdF2cf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q pytubefix\n",
        "!pip uninstall -q pytube\n",
        "!pip install -q youtube-transcript-api\n",
        "!pip install -q anthropic\n",
        "!pip install faster_whisper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx6v-ap8GK5X"
      },
      "source": [
        "Let us load the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SZueZdDuE5F0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "import pytubefix as pytube\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from faster_whisper import WhisperModel\n",
        "import torch\n",
        "import anthropic\n",
        "\n",
        "import cv2 #Used to extract frames from video\n",
        "import base64 #Used to convert JPG image in base64 format\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MHgGIObGUst"
      },
      "source": [
        "Put your Anthropic API key here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2KTQK1usGUzu"
      },
      "outputs": [],
      "source": [
        "ANTHROPIC_API_KEY = \"sk-ant-xxx\"\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
        "\n",
        "client = anthropic.Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R354_5FI2x-e"
      },
      "source": [
        "Define Youtube video ID, folder to store video, chapters, and resulting blog post."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5jXfMMx1H1Ag"
      },
      "outputs": [],
      "source": [
        "# Andrej Karpathy : Let's build the GPT Tokenizer - https://www.youtube.com/watch?v=zduSFxRajkE\n",
        "youtube_video_id = \"zduSFxRajkE\"\n",
        "\n",
        "DATA_DIR = youtube_video_id\n",
        "CHAPTERS_DIR = DATA_DIR+\"/chapters\"\n",
        "MERGE_DIR = DATA_DIR+\"/final_output\"\n",
        "\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    os.makedirs(DATA_DIR)\n",
        "\n",
        "if not os.path.exists(CHAPTERS_DIR):\n",
        "    os.makedirs(CHAPTERS_DIR)\n",
        "\n",
        "if not os.path.exists(MERGE_DIR):\n",
        "    os.makedirs(MERGE_DIR)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is2gMyoUG7Pr"
      },
      "source": [
        "## Download video and get transcript\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJzyyDGqHNho"
      },
      "source": [
        "### Download video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6oRHJWUfHHXO"
      },
      "outputs": [],
      "source": [
        "import pytubefix as pytube\n",
        "\n",
        "def download_youtube_video(video_id, output_path):\n",
        "    \"\"\"\n",
        "    Download a YouTube video given its ID, stores it in output_path, and returns the output path with the video ID as filename.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a YouTube object with the video ID\n",
        "    youtube = pytube.YouTube(f\"https://www.youtube.com/watch?v={video_id}\")\n",
        "    # Get the highest resolution video stream\n",
        "    stream = youtube.streams.get_highest_resolution()\n",
        "    # Download the video\n",
        "    video_path = stream.download(output_path=output_path, filename=video_id+\".mp4\")\n",
        "\n",
        "    return video_path\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JPOOVwlf7nSF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMSL7OY3HSts",
        "outputId": "433a3bec-b382-4ba0-c7b1-6e1553703b9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 393 ms, sys: 408 ms, total: 800 ms\n",
            "Wall time: 10 s\n"
          ]
        }
      ],
      "source": [
        "# About 20 seconds for 330MB video\n",
        "%time video_path=download_youtube_video(youtube_video_id, DATA_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l10eoDUI5MB"
      },
      "source": [
        "### Get transcript\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wOGlSZh2x-f"
      },
      "source": [
        "#### With YouTubeTranscriptApi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3tHUAQ-2x-f",
        "outputId": "96a601cb-c6be-4d3b-d3de-f15d324db917"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': \"hi everyone so in this video I'd like us\",\n",
              "  'start': 0.04,\n",
              "  'duration': 4.04},\n",
              " {'text': 'to cover the process of tokenization in',\n",
              "  'start': 2.04,\n",
              "  'duration': 4.4},\n",
              " {'text': 'large language models now you see here',\n",
              "  'start': 4.08,\n",
              "  'duration': 4.2},\n",
              " {'text': \"that I have a set face and that's\",\n",
              "  'start': 6.44,\n",
              "  'duration': 3.88}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "transcript = YouTubeTranscriptApi.get_transcript(youtube_video_id, languages=[\"en\"])\n",
        "transcript[0:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdzWk2bT2x-f",
        "outputId": "a5bd0080-da30-46aa-bcfe-3a6d9982c82a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3422"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "len(transcript)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkr6Z79-2x-f"
      },
      "source": [
        "#### With Whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WoCYKdXIm6t2"
      },
      "outputs": [],
      "source": [
        "whisper_model = WhisperModel(\"large-v3\",\n",
        "                              device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "                              #compute_type=\"float16\",\n",
        "                            )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CHCWZ4OI7_Sv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sYZT9QZHn19t"
      },
      "outputs": [],
      "source": [
        "def speech_to_text(whisper_model, audio_file, initial_prompt=\"\", language=\"en\", segments=None):\n",
        "\n",
        "        segments, transcript_info = whisper_model.transcribe(audio_file,  initial_prompt=initial_prompt, language=language)\n",
        "        segments = list(segments)\n",
        "        segments = [\n",
        "            {\n",
        "                \"start\": round(s.start,2),\n",
        "                \"duration\": round(s.end-s.start,2),\n",
        "                \"text\": s.text,\n",
        "            }\n",
        "            for s in segments\n",
        "        ]\n",
        "\n",
        "        return segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dheVHMrj2x-f"
      },
      "outputs": [],
      "source": [
        "#25 minutes for a 2h13 video on T4\n",
        "transcript = speech_to_text(whisper_model, video_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geO0dtBmHHfv"
      },
      "source": [
        "## Chop up in chapters of aligned text and screenshots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7oNw6Cg2x-f"
      },
      "outputs": [],
      "source": [
        "def get_text_chapter(transcript, chapter_start_time, chapter_end_time, output_dir):\n",
        "    \"\"\"\n",
        "    Extract and save a specified chapter's text from a transcript.\n",
        "\n",
        "    This function iterates through a transcript, extracting text that falls within the specified start and end times\n",
        "    of a chapter. The extracted text is concatenated into a single string, which is then saved to a file named\n",
        "    'transcript.txt' within the specified output directory.\n",
        "\n",
        "    Args:\n",
        "        transcript (list of dicts): The transcript from which to extract text, where each entry in the list\n",
        "            represents a segment of the transcript with a start time, end time, and text.\n",
        "        chapter_start_time (int): The start time of the chapter, used to identify which segments of the transcript to include.\n",
        "        chapter_end_time (int): The end time of the chapter, used to identify which segments of the transcript to include.\n",
        "        output_dir (str): The directory where the extracted chapter text will be saved.\n",
        "\n",
        "    The function does not return any value but writes the extracted chapter text to 'transcript.txt' in the specified directory.\n",
        "    \"\"\"\n",
        "    text_chapter = \"\"\n",
        "\n",
        "    for i in range(len(transcript)):\n",
        "        transcript_i = transcript[i]\n",
        "\n",
        "        # Check if the current transcript segment falls within the chapter's start and end times\n",
        "        if int(transcript_i['start']) >= chapter_start_time and int(transcript_i['start']) <= chapter_end_time:\n",
        "            # Concatenate text from the audio transcript, removing any new lines and leading/trailing whitespace\n",
        "            text_chapter += transcript_i['text'].replace('\\n', ' ').strip() + \" \"\n",
        "\n",
        "    # Define the path to the output transcript file\n",
        "    transcript_file = output_dir + '/transcript.txt'\n",
        "\n",
        "    # Save the concatenated chapter text to the specified file\n",
        "    with open(transcript_file, \"w\") as f:\n",
        "        f.write(text_chapter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMb9zq8r2x-f"
      },
      "outputs": [],
      "source": [
        "def get_frames_chapter(video_path, chapter_start_time, chapter_end_time, output_dir, timestamps_screenshots=None):\n",
        "    \"\"\"\n",
        "    Extract and save frames from a specified chapter of a video at given timestamps or at regular intervals.\n",
        "\n",
        "    This function calculates a list of timestamps to take screenshots if not provided, defaulting to 10 evenly spaced\n",
        "    intervals within the chapter duration. If the calculated interval is less than 60 seconds, it defaults to 60 seconds.\n",
        "    It then opens the video file, iterates over the calculated or provided timestamps, captures frames at these timestamps,\n",
        "    and saves them as JPEG files in the specified output directory.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): The path to the video file.\n",
        "        chapter_start_time (int): The start time of the chapter in seconds.\n",
        "        chapter_end_time (int): The end time of the chapter in seconds.\n",
        "        output_dir (str): The directory where the extracted frames will be saved.\n",
        "        timestamps_screenshots (list of int, optional): Specific timestamps to capture screenshots. If None,\n",
        "            screenshots will be taken at regular intervals within the chapter.\n",
        "\n",
        "    The function does not return any value but saves the captured frames in the specified output directory.\n",
        "    \"\"\"\n",
        "    # Calculate default timestamps if not provided\n",
        "    if timestamps_screenshots is None:\n",
        "        screenshot_interval = int((chapter_end_time - chapter_start_time) / 10)\n",
        "        # Ensure a minimum interval of 60 seconds between screenshots\n",
        "        if screenshot_interval < 60:\n",
        "            screenshot_interval = 60\n",
        "        timestamps_screenshots = list(range(chapter_start_time, chapter_end_time, screenshot_interval))\n",
        "    else:\n",
        "        #Make sure timestamps are integers\n",
        "        timestamps_screenshots = [int(ts) for ts in timestamps_screenshots]\n",
        "\n",
        "    # Open the video file using OpenCV\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Determine the frames per second (FPS) of the video for frame index calculation\n",
        "    fps = video.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    # Capture and save frames at each specified timestamp\n",
        "    for timestamp in timestamps_screenshots:\n",
        "        # Calculate the frame index based on the timestamp and video FPS\n",
        "        index = int(timestamp * fps)\n",
        "        video.set(cv2.CAP_PROP_POS_FRAMES, index)\n",
        "\n",
        "        # Attempt to read the frame at the calculated index\n",
        "        success, frame = video.read()\n",
        "\n",
        "        # If the frame is successfully read, save it as a JPEG file\n",
        "        if success:\n",
        "            # Format the timestamp for the output filename\n",
        "            timestamp_str = \"{:05d}\".format(timestamp)\n",
        "            output_path = f\"{output_dir}/{timestamp_str}.jpg\"\n",
        "            # Save the frame to the output directory\n",
        "            cv2.imwrite(output_path, frame)\n",
        "\n",
        "    # Release the video file resources\n",
        "    video.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdYRDKiLhcFy"
      },
      "outputs": [],
      "source": [
        "def chop_up_in_chapters(chapters_list, video_path, transcript, timestamps_screenshots_list_seconds=None):\n",
        "    \"\"\"\n",
        "    Split the video in chapters based on the video chapters list.\n",
        "    \"\"\"\n",
        "\n",
        "    n_chapters=len(chapters_list)-1\n",
        "    print(f\"Number of chunks: {n_chapters}\")\n",
        "\n",
        "    # Iterate over the timestamps and topics\n",
        "    for current_chapter in range(n_chapters):\n",
        "\n",
        "        output_dir=CHAPTERS_DIR+\"/\"+str(current_chapter)\n",
        "\n",
        "         # Create the output directory if it does not exist\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        # Get the current and next timestamp\n",
        "        current_chunk_start_time=chapters_list[current_chapter]['timestamp']\n",
        "        current_chunk_end_time=chapters_list[current_chapter+1]['timestamp']-1\n",
        "\n",
        "        print(f\"Chapter {current_chapter}; Start: {current_chunk_start_time}, End: {current_chunk_end_time}\")\n",
        "\n",
        "        # Extract text and frames for the current chapter\n",
        "        get_text_chapter(transcript, current_chunk_start_time, current_chunk_end_time, output_dir)\n",
        "\n",
        "        if timestamps_screenshots_list_seconds is not None:\n",
        "            get_frames_chapter(video_path, current_chunk_start_time, current_chunk_end_time, output_dir,timestamps_screenshots_list_seconds[current_chapter])\n",
        "        else:\n",
        "            get_frames_chapter(video_path, current_chunk_start_time, current_chunk_end_time, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jogen2jq2x-f",
        "outputId": "f4e3accd-56b9-4619-bbc0-14d72d2c8d8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'timestamp': 0,\n",
              "  'topic': 'intro: Tokenization, GPT-2 paper, tokenization-related issues'},\n",
              " {'timestamp': 350,\n",
              "  'topic': 'tokenization by example in a Web UI (tiktokenizer)'},\n",
              " {'timestamp': 896, 'topic': 'strings in Python, Unicode code points'},\n",
              " {'timestamp': 1095,\n",
              "  'topic': 'Unicode byte encodings, ASCII, UTF-8, UTF-16, UTF-32'},\n",
              " {'timestamp': 1367, 'topic': 'daydreaming: deleting tokenization'},\n",
              " {'timestamp': 1430,\n",
              "  'topic': 'Byte Pair Encoding (BPE) algorithm walkthrough'},\n",
              " {'timestamp': 1622, 'topic': 'starting the implementation'},\n",
              " {'timestamp': 1715,\n",
              "  'topic': 'counting consecutive pairs, finding most common pair'},\n",
              " {'timestamp': 1836, 'topic': 'merging the most common pair'},\n",
              " {'timestamp': 2098,\n",
              "  'topic': 'training the tokenizer: adding the while loop, compression ratio'},\n",
              " {'timestamp': 2360,\n",
              "  'topic': 'tokenizer/LLM diagram: it is a completely separate stage'},\n",
              " {'timestamp': 2567, 'topic': 'decoding tokens to strings'},\n",
              " {'timestamp': 2901, 'topic': 'encoding strings to tokens'},\n",
              " {'timestamp': 3456,\n",
              "  'topic': 'regex patterns to force splits across categories'},\n",
              " {'timestamp': 4298,\n",
              "  'topic': 'tiktoken library intro, differences between GPT-2/GPT-4 regex'},\n",
              " {'timestamp': 4499,\n",
              "  'topic': 'GPT-2 encoder.py released by OpenAI walkthrough'},\n",
              " {'timestamp': 4706,\n",
              "  'topic': 'special tokens, tiktoken handling of, GPT-2/GPT-4 differences'},\n",
              " {'timestamp': 5128,\n",
              "  'topic': 'minbpe exercise time! write your own GPT-4 tokenizer'},\n",
              " {'timestamp': 5322,\n",
              "  'topic': 'sentencepiece library intro, used to train Llama 2 vocabulary'},\n",
              " {'timestamp': 6207,\n",
              "  'topic': 'how to set vocabulary set? revisiting gpt.py transformer'},\n",
              " {'timestamp': 6491,\n",
              "  'topic': 'training new tokens, example of prompt compression'},\n",
              " {'timestamp': 6598,\n",
              "  'topic': 'multimodal [image, video, audio] tokenization with vector quantization'},\n",
              " {'timestamp': 6701,\n",
              "  'topic': 'revisiting and explaining the quirks of LLM tokenization'},\n",
              " {'timestamp': 7820, 'topic': 'final recommendations'},\n",
              " {'timestamp': 7970, 'topic': '??? :)'}]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chapters_24=\"\"\"\n",
        "00:00:00 intro: Tokenization, GPT-2 paper, tokenization-related issues\n",
        "00:05:50 tokenization by example in a Web UI (tiktokenizer)\n",
        "00:14:56 strings in Python, Unicode code points\n",
        "00:18:15 Unicode byte encodings, ASCII, UTF-8, UTF-16, UTF-32\n",
        "00:22:47 daydreaming: deleting tokenization\n",
        "00:23:50 Byte Pair Encoding (BPE) algorithm walkthrough\n",
        "00:27:02 starting the implementation\n",
        "00:28:35 counting consecutive pairs, finding most common pair\n",
        "00:30:36 merging the most common pair\n",
        "00:34:58 training the tokenizer: adding the while loop, compression ratio\n",
        "00:39:20 tokenizer/LLM diagram: it is a completely separate stage\n",
        "00:42:47 decoding tokens to strings\n",
        "00:48:21 encoding strings to tokens\n",
        "00:57:36 regex patterns to force splits across categories\n",
        "01:11:38 tiktoken library intro, differences between GPT-2/GPT-4 regex\n",
        "01:14:59 GPT-2 encoder.py released by OpenAI walkthrough\n",
        "01:18:26 special tokens, tiktoken handling of, GPT-2/GPT-4 differences\n",
        "01:25:28 minbpe exercise time! write your own GPT-4 tokenizer\n",
        "01:28:42 sentencepiece library intro, used to train Llama 2 vocabulary\n",
        "01:43:27 how to set vocabulary set? revisiting gpt.py transformer\n",
        "01:48:11 training new tokens, example of prompt compression\n",
        "01:49:58 multimodal [image, video, audio] tokenization with vector quantization\n",
        "01:51:41 revisiting and explaining the quirks of LLM tokenization\n",
        "02:10:20 final recommendations\n",
        "02:12:50 ??? :)\n",
        "\"\"\"\n",
        "\n",
        "def chapters_to_list(chapters):\n",
        "    chapters_list = chapters.strip().split('\\n')\n",
        "    chapters_dict_list = []\n",
        "\n",
        "    for chapter in chapters_list:\n",
        "        time_str, topic = chapter.split(' ', 1)\n",
        "        hours, minutes, seconds = map(int, time_str.split(':'))\n",
        "        total_seconds = hours * 3600 + minutes * 60 + seconds\n",
        "        chapters_dict_list.append({\"timestamp\": total_seconds, \"topic\": topic})\n",
        "\n",
        "    return chapters_dict_list\n",
        "\n",
        "chapters_list = chapters_to_list(chapters_24)\n",
        "last_timestamp=int(transcript[-1]['start']+transcript[-1]['duration'])\n",
        "#chapters_list.append({\"timestamp\": last_timestamp, \"topic\": \"end\"})\n",
        "chapters_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZj-HZkxhcFy",
        "outputId": "0077689e-877f-4a5b-f4c2-54e541202918"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of chunks: 24\n",
            "Chapter 0; Start: 0, End: 349\n",
            "Chapter 1; Start: 350, End: 895\n",
            "Chapter 2; Start: 896, End: 1094\n",
            "Chapter 3; Start: 1095, End: 1366\n",
            "Chapter 4; Start: 1367, End: 1429\n",
            "Chapter 5; Start: 1430, End: 1621\n",
            "Chapter 6; Start: 1622, End: 1714\n",
            "Chapter 7; Start: 1715, End: 1835\n",
            "Chapter 8; Start: 1836, End: 2097\n",
            "Chapter 9; Start: 2098, End: 2359\n",
            "Chapter 10; Start: 2360, End: 2566\n",
            "Chapter 11; Start: 2567, End: 2900\n",
            "Chapter 12; Start: 2901, End: 3455\n",
            "Chapter 13; Start: 3456, End: 4297\n",
            "Chapter 14; Start: 4298, End: 4498\n",
            "Chapter 15; Start: 4499, End: 4705\n",
            "Chapter 16; Start: 4706, End: 5127\n",
            "Chapter 17; Start: 5128, End: 5321\n",
            "Chapter 18; Start: 5322, End: 6206\n",
            "Chapter 19; Start: 6207, End: 6490\n",
            "Chapter 20; Start: 6491, End: 6597\n",
            "Chapter 21; Start: 6598, End: 6700\n",
            "Chapter 22; Start: 6701, End: 7819\n",
            "Chapter 23; Start: 7820, End: 7969\n"
          ]
        }
      ],
      "source": [
        "chop_up_in_chapters(chapters_list, video_path, transcript)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e9CowTGK_QY"
      },
      "source": [
        "## LLM transform\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mdP9bU_LEXO"
      },
      "source": [
        "This is the core step. For each chapter, the audio transcript and selected screenshots are provided to the LMM, with the goal of transforming these input data into an output suitable for inclusion in a textbook.\n",
        "\n",
        "Documentation for querying Claude with images: https://docs.anthropic.com/claude/docs/vision\n",
        "\n",
        "Prompt inspired by https://github.com/hundredblocks/transcription_demo/tree/main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OSZm1gn2x-g"
      },
      "outputs": [],
      "source": [
        "prompt_instructions = f\"\"\"\n",
        "<instructions>\n",
        "You have been given images of a video at different timestamps, followed by the audio transcript in <transcript>\n",
        "The transcript was generated by an AI speech recognition tool and may contain some errors/infelicities.\n",
        "Your task is to transform the transcript into a markdown blog post.\n",
        "This transcript is noisy. Please rewrite it using the following guidelines:\n",
        "- output valid markdown\n",
        "- insert section headings and other formatting where appropriate\n",
        "- you are given only part of a transcript, so do not include introductory or concluding paragraphs. Only include the main topics discussed in the transcript\n",
        "- use styling to make images, text, code, callouts and the page layout and margins look like a typical blog post or textbook\n",
        "- remove any verbal tics\n",
        "- if there are redundant pieces of information, only present it once\n",
        "- keep the conversational content in the style of the transcript. Including headings to make the narrative structure easier to follow along\n",
        "- the transcript includes too many images, so you should only include the most important 1-2 images in your output\n",
        "- choose images that provide illustrations that are relevant to the transcript\n",
        "- prefer to include images which display complete code, rather than in progress\n",
        "- when relevant transcribe important pieces of code and other valuable text\n",
        "- if an image would help illustrate a part of a transcript, include it\n",
        "- to include an image, insert a tag with <img src=\"xxxxx.jpg\"/> where xxxxx is replaced by the exact image timestamp inserted above the image data\n",
        "- do not add any extraneous information: only include what is either mentioned in the transcript or the images\n",
        "\n",
        "Your final output should be suitable for inclusion in a textbook.\n",
        "</instructions>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj17pv5w2x-g"
      },
      "source": [
        "Transform the JPG screenshots in a format suitable for Anthorpic's API.\n",
        "\n",
        "The function iterates over all screenshots in order to describe each of them with two messages:\n",
        "\n",
        "- a text message that specifies the timestamp for the screenshot,\n",
        "- and an image message containing its base64-encoded representation.\n",
        "\n",
        "The text message with the timestamp will allow later to add a hyperlink from the final document to the original video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyDpiSUKLRz1"
      },
      "outputs": [],
      "source": [
        "def get_screenshots_as_messages(screenshots):\n",
        "\n",
        "\tscreenshots_as_messages = []\n",
        "\n",
        "\tfor i in range(len(screenshots)):\n",
        "\t\tscreenshots_as_messages.extend([\n",
        "\t\t{\n",
        "\t\t\t\"type\": \"text\",\n",
        "\t\t\t\"text\": f\"The timestamp for the following image is {Path(screenshots[i]).stem}.\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\"type\": \"image\",\n",
        "\t\t\"source\": {\n",
        "\t\t\t\"type\": \"base64\",\n",
        "\t\t\t\"media_type\": \"image/jpeg\",\n",
        "\t\t\t\"data\": base64.b64encode(open(screenshots[i], \"rb\").read()).decode(\"utf-8\"),\n",
        "\t\t}\n",
        "\t\t}\n",
        "\t\t])\n",
        "\n",
        "return screenshots_as_messages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yOt4dEJ2x-g"
      },
      "source": [
        "Bring together the screenshots, transcript and instructions.\n",
        "\n",
        "The function additionally prefills Claude's output to make it start its answer with a markdown title - https://docs.anthropic.com/claude/docs/prefill-claudes-response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftRn7EiD2x-g"
      },
      "outputs": [],
      "source": [
        "def get_prompt_as_messages(chapter_id):\n",
        "\n",
        "    folder_path=CHAPTERS_DIR+'/'+str(chapter_id)\n",
        "\n",
        "    with open(folder_path+'/transcript.txt', \"r\") as f:\n",
        "        transcript = f.read()\n",
        "\n",
        "    screenshots=sorted(glob.glob(folder_path+'/*.jpg'))\n",
        "\n",
        "    screenshots_as_messages=get_screenshots_as_messages(screenshots)\n",
        "\n",
        "    prompt_as_messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": screenshots_as_messages+\n",
        "            [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": f\"<transcript>\\n{transcript}\\n</transcript>\"\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": prompt_instructions\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"#\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    return prompt_as_messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owv_YpF7hcFz"
      },
      "outputs": [],
      "source": [
        "# Check content\n",
        "prompt_as_messages = get_prompt_as_messages(0)\n",
        "prompt_as_messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa4jmJsU2x-g"
      },
      "source": [
        "Iteratively call Claude, and writing the result as a markdown file in the corresponding chapter folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0FJLgk0hcFz",
        "outputId": "ad039dc0-9699-4562-b3b0-44748967a4fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing chunk 0\n",
            "Processing chunk 1\n",
            "Processing chunk 2\n",
            "Processing chunk 3\n",
            "Processing chunk 4\n",
            "Processing chunk 5\n",
            "Processing chunk 6\n",
            "Processing chunk 7\n",
            "Processing chunk 8\n",
            "Processing chunk 9\n",
            "Processing chunk 10\n",
            "Processing chunk 11\n",
            "Processing chunk 12\n",
            "Processing chunk 13\n",
            "Processing chunk 14\n",
            "Processing chunk 15\n",
            "Processing chunk 16\n",
            "Processing chunk 17\n",
            "Processing chunk 18\n",
            "Processing chunk 19\n",
            "Processing chunk 20\n",
            "Processing chunk 21\n",
            "Processing chunk 22\n",
            "Processing chunk 23\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Iterate through the list of chapters\n",
        "for chapter in range(len(chapters_list)-1):\n",
        "\n",
        "    # Display the current processing chapter number to the console.\n",
        "    print(f\"Processing chunk {chapter}\")\n",
        "\n",
        "    # Generate the prompt for the current chapter (list of messages with screenshots, transcript and instructions).\n",
        "    prompt_generate_markdown = get_prompt_as_messages(chapter)\n",
        "\n",
        "    # Create a message by invoking Claude with the prompt.\n",
        "    message = client.messages.create(\n",
        "        model=\"claude-3-opus-20240229\",\n",
        "        system=\"You are an expert at writing markdown blog post.\",\n",
        "        temperature=0,\n",
        "        max_tokens=4000,\n",
        "        messages=prompt_generate_markdown\n",
        "    )\n",
        "\n",
        "    # Extract the generated markdown content from the response.\n",
        "    answer = message.content[0].text\n",
        "    markdown = \"#\"+answer  # Prepend a header tag to the markdown content.\n",
        "\n",
        "    # Define the path for the markdown file corresponding to the current chapter.\n",
        "    markdown_file = CHAPTERS_DIR + '/' + str(chapter) + '/markdown.md'\n",
        "\n",
        "    # Write the generated markdown content to the file.\n",
        "    with open(markdown_file, \"w\") as f:\n",
        "        f.write(markdown)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQFPDtlk2x-g"
      },
      "source": [
        "## Merge all chapters and finalize blog post\n",
        "\n",
        "The final and last step of the workflow consists in two main tasks. First, it merges together the different markdown outputs. Second, it adds hyperlinks to chapter titles and images. This allows to connect the final markdown file to the original YouTube video at relevant timestamps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MdV06shhcFz"
      },
      "outputs": [],
      "source": [
        "merged_markdown=\"\"\n",
        "\n",
        "# Iterate over the chapter folders to merge the markdown files\n",
        "for chapter in range(len(chapters_list)-1):\n",
        "\n",
        "    markdown_file=CHAPTERS_DIR+'/'+str(chapter)+'/markdown.md'\n",
        "\n",
        "    with open(markdown_file, \"r\") as f:\n",
        "        markdown = f.readlines()\n",
        "\n",
        "    # Let us add, for each chapter title, a hyperlink to the video at the right timestamp\n",
        "    url_chapter = f\"https://www.youtube.com/watch?v={youtube_video_id}&t={chapters_list[chapter]['timestamp']}s\"\n",
        "    markdown[0] = f\"# [{chapter+1}) {markdown[0][2:].strip()}]({url_chapter})\"\n",
        "    markdown = '\\n'.join(markdown)\n",
        "\n",
        "    merged_markdown+=\"\\n\"+markdown\n",
        "\n",
        "# Find all <img> tags with timestamps in the src attribute, so we can add a hyperlink to the video at the right timestamp\n",
        "timestamps_screenshots = re.findall(r'<img src=\"(\\d+)\\.jpg\"/>', merged_markdown)\n",
        "timestamps_screenshots = [timestamp for timestamp in timestamps_screenshots]\n",
        "\n",
        "# Add a hyperlink to the video at the right timestamp for each image\n",
        "for timestamp in timestamps_screenshots:\n",
        "    video_link = f'<a href=\"https://www.youtube.com/watch?v={youtube_video_id}&t={int(timestamp)}s\">Link to video</a>'\n",
        "    merged_markdown = merged_markdown.replace(f'<img src=\"{timestamp}.jpg\"/>', f'<img src=\"{timestamp}.jpg\"/>\\n\\n{video_link}')\n",
        "\n",
        "# Get frames based on screenshots effectively selected in the merged markdown and save in merge folder\n",
        "get_frames_chapter(video_path, None, None, MERGE_DIR, timestamps_screenshots=timestamps_screenshots)\n",
        "\n",
        "# Save the merged markdown to a markdown blogpost.md file\n",
        "markdown_file=MERGE_DIR+'/blogpost.md'\n",
        "with open(markdown_file, \"w\") as f:\n",
        "        f.write(merged_markdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbw-lUaYKUR9"
      },
      "source": [
        "## Useful links\n",
        "\n",
        "- Companion Medium article\n",
        "- [Claude 3 - Vision documentation](https://docs.anthropic.com/claude/docs/vision)\n",
        "- Karpathy's challenge and Ameisen and colleague's repository\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}